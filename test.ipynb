{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "def create_log_gaussian(mean, log_std, t):\n",
    "    quadratic = -((0.5 * (t - mean) / (log_std.exp())).pow(2))\n",
    "    l = mean.shape\n",
    "    log_z = log_std\n",
    "    z = l[-1] * math.log(2 * math.pi)\n",
    "    log_p = quadratic.sum(dim=-1) - log_z.sum(dim=-1) - 0.5 * z\n",
    "    return log_p\n",
    "\n",
    "def logsumexp(inputs, dim=None, keepdim=False):\n",
    "    if dim is None:\n",
    "        inputs = inputs.view(-1)\n",
    "        dim = 0\n",
    "    s, _ = torch.max(inputs, dim=dim, keepdim=True)\n",
    "    outputs = s + (inputs - s).exp().sum(dim=dim, keepdim=True).log()\n",
    "    if not keepdim:\n",
    "        outputs = outputs.squeeze(dim)\n",
    "    return outputs\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "epsilon = 1e-6\n",
    "\n",
    "# Initialize Policy weights\n",
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.linear4 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
    "        self.linear5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear6 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        xu = torch.cat([state, action], 1)\n",
    "        \n",
    "        x1 = F.relu(self.linear1(xu))\n",
    "        x1 = F.relu(self.linear2(x1))\n",
    "        x1 = self.linear3(x1)\n",
    "\n",
    "        x2 = F.relu(self.linear4(xu))\n",
    "        x2 = F.relu(self.linear5(x2))\n",
    "        x2 = self.linear6(x2)\n",
    "\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.mean_linear = nn.Linear(hidden_dim, num_actions)\n",
    "        self.log_std_linear = nn.Linear(hidden_dim, num_actions)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mean = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        action = torch.tanh(x_t)\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(1 - action.pow(2) + epsilon)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        return action, log_prob, torch.tanh(mean)\n",
    "\n",
    "class DeterministicPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "        super(DeterministicPolicy, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.mean = nn.Linear(hidden_dim, num_actions)\n",
    "        self.noise = torch.Tensor(num_actions)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mean = torch.tanh(self.mean(x))\n",
    "        return mean\n",
    "\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean = self.forward(state)\n",
    "        noise = self.noise.normal_(0., std=0.1)\n",
    "        noise = noise.clamp(-0.25, 0.25)\n",
    "        action = mean + noise\n",
    "        return action, torch.tensor(0.), mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "class SAC(object):\n",
    "    def __init__(self, num_inputs, action_space, gamma, tau, alpha, policy, target_update_interval, automatic_entropy_tuning, hidden_size, lr):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.action_range = [action_space.low, action_space.high]\n",
    "\n",
    "        self.policy_type = policy\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.automatic_entropy_tuning = automatic_entropy_tuning\n",
    "\n",
    "        self.device = torch.device(\"cuda\") \n",
    "\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], hidden_size).to(device=self.device)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=lr)\n",
    "\n",
    "        self.critic_target = QNetwork(num_inputs, action_space.shape[0], hidden_size).to(self.device)\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        if self.policy_type == \"Gaussian\":\n",
    "            # Target Entropy = ‚àídim(A) (e.g. , -6 for HalfCheetah-v2) as given in the paper\n",
    "            if self.automatic_entropy_tuning == True:\n",
    "                self.target_entropy = -torch.prod(torch.Tensor(action_space.shape).to(self.device)).item()\n",
    "                self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "                self.alpha_optim = Adam([self.log_alpha], lr=lr)\n",
    "\n",
    "\n",
    "            self.policy = GaussianPolicy(num_inputs, action_space.shape[0], hidden_size).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        else:\n",
    "            self.alpha = 0\n",
    "            self.automatic_entropy_tuning = False\n",
    "            self.policy = DeterministicPolicy(num_inputs, action_space.shape[0], hidden_size).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "    def select_action(self, state, eval=False):\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        if eval == False:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        else:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        action = action.detach().cpu().numpy()[0]\n",
    "        return self.rescale_action(action)\n",
    "\n",
    "    def rescale_action(self, action):\n",
    "        return action * (self.action_range[1] - self.action_range[0]) / 2.0 + (self.action_range[1] + self.action_range[0]) / 2.0\n",
    "\n",
    "    def update_parameters(self, memory, batch_size, updates):\n",
    "        # Sample a batch from memory\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch, truncated_batch = memory.sample(batch_size=batch_size)\n",
    "\n",
    "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_action, next_state_log_pi, _ = self.policy.sample(next_state_batch)\n",
    "            qf1_next_target, qf2_next_target = self.critic_target(next_state_batch, next_state_action)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_pi\n",
    "            next_q_value = reward_batch + mask_batch * self.gamma * (min_qf_next_target)\n",
    "\n",
    "        qf1, qf2 = self.critic(state_batch, action_batch)  # Two Q-functions to mitigate positive bias in the policy improvement step\n",
    "        qf1_loss = F.mse_loss(qf1, next_q_value) # JQ = ùîº(st,at)~D[0.5(Q1(st,at) - r(st,at) - Œ≥(ùîºst+1~p[V(st+1)]))^2]\n",
    "        qf2_loss = F.mse_loss(qf2, next_q_value) # JQ = ùîº(st,at)~D[0.5(Q1(st,at) - r(st,at) - Œ≥(ùîºst+1~p[V(st+1)]))^2]\n",
    "\n",
    "        pi, log_pi, _ = self.policy.sample(state_batch)\n",
    "\n",
    "        qf1_pi, qf2_pi = self.critic(state_batch, pi)\n",
    "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "\n",
    "        policy_loss = ((self.alpha * log_pi) - min_qf_pi).mean() # JœÄ = ùîºst‚àºD,Œµt‚àºN[Œ± * logœÄ(f(Œµt;st)|st) ‚àí Q(st,f(Œµt;st))]\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        qf1_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        qf2_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "\n",
    "            self.alpha_optim.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optim.step()\n",
    "\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "            alpha_tlogs = self.alpha.clone() # For TensorboardX logs\n",
    "        else:\n",
    "            alpha_loss = torch.tensor(0.).to(self.device)\n",
    "            alpha_tlogs = torch.tensor(self.alpha) # For TensorboardX logs\n",
    "\n",
    "\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            soft_update(self.critic_target, self.critic, self.tau)\n",
    "\n",
    "        return qf1_loss.item(), qf2_loss.item(), policy_loss.item(), alpha_loss.item(), alpha_tlogs.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done, truncated):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done, truncated)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done, truncated = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done, truncated\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"HalfCheetah-v2\")\n",
    "\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "action_space = env.action_space\n",
    "gamma = 0.99\n",
    "tau = 0.95\n",
    "alpha =0.2\n",
    "policy = \"Gaussian\"\n",
    "target_update_interval = 1\n",
    "automatic_entropy_tuning = True\n",
    "hidden_size = 256\n",
    "lr = 3e-4\n",
    "replay_size = 1000000\n",
    "batch_size = 512\n",
    "updates_per_step = 1\n",
    "num_steps = 1000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SAC(env.observation_space.shape[0], env.action_space, gamma, tau, alpha, policy, target_update_interval, automatic_entropy_tuning, hidden_size, lr)\n",
    "memory = ReplayMemory(replay_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_numsteps = 0\n",
    "updates = 0\n",
    "#env.render()\n",
    "for i_episode in itertools.count(1):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "    state, _ = env.reset()\n",
    "    #print(state)\n",
    "\n",
    "    while not (done, truncated):\n",
    "        #env.render()\n",
    "        '''\n",
    "        if args.start_steps > total_numsteps:\n",
    "            action = env.action_space.sample()  # Sample random action\n",
    "        else:\n",
    "            action = agent.select_action(state)  # Sample action from policy\n",
    "        '''\n",
    "        action = agent.select_action(state)  # Sample action from policy\n",
    "        #print(action)\n",
    "        #assert 0==1\n",
    "        if len(memory) > batch_size:\n",
    "            # Number of updates per step in environment\n",
    "            for i in range(updates_per_step):\n",
    "                # Update parameters of all the networks\n",
    "                critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, batch_size, updates)\n",
    "\n",
    "                #writer.add_scalar('loss/critic_1', critic_1_loss, updates)\n",
    "                #writer.add_scalar('loss/critic_2', critic_2_loss, updates)\n",
    "                #writer.add_scalar('loss/policy', policy_loss, updates)\n",
    "                #writer.add_scalar('loss/entropy_loss', ent_loss, updates)\n",
    "                #writer.add_scalar('entropy_temprature/alpha', alpha, updates)\n",
    "                updates += 1\n",
    "\n",
    "        next_state, reward, done, truncated, _ = env.step(action) # Step\n",
    "\n",
    "        episode_steps += 1\n",
    "        total_numsteps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time horizon.\n",
    "        # (https://github.com/openai/spinningup/blob/master/spinup/algos/sac/sac.py)\n",
    "        mask = 1 if episode_steps == env._max_episode_steps else float(not done)\n",
    "\n",
    "        memory.push(state, action, reward, next_state, mask, truncated) # Append transition to memory\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    if total_numsteps > num_steps:\n",
    "        break\n",
    "\n",
    "    #writer.add_scalar('reward/train', episode_reward, i_episode)\n",
    "    print(\"Episode: {}, total numsteps: {}, episode steps: {}, reward: {}\".format(i_episode, total_numsteps, episode_steps, round(episode_reward, 2)))\n",
    "    #logger.info('Env %d, Goal (%05.1f, %05.1f), Episode %05d, setp %03d, Reward %-5.1f,%s' % (env.index, env.goal_point[0], env.goal_point[1], id + 1, step, ep_reward, result))\n",
    "    #logger_cal.info(episode_reward)\n",
    "\n",
    "    '''\n",
    "    if i_episode % 10 == 0 and args.eval == True:\n",
    "        agent.save_model(suffix=i_episode)\n",
    "        #print('save model succeed!')\n",
    "        avg_reward = 0.\n",
    "        episodes = 10\n",
    "        for _  in range(episodes):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                #env.render()\n",
    "                action = agent.select_action(state, eval=True)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "            avg_reward += episode_reward\n",
    "        avg_reward /= episodes\n",
    "        \n",
    "        #writer.add_scalar('avg_reward/test', avg_reward, i_episode)\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Test Episodes: {}, Avg. Reward: {}\".format(episodes, round(avg_reward, 2)))\n",
    "        print(\"----------------------------------------\")\n",
    "    '''\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
